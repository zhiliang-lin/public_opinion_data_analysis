{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') #忽视警告\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 中文字体设置-黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"douban_queen_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载外部分词字典\n",
    "jieba.load_userdict(\"userdict.txt\")\n",
    "\n",
    "# 使用jieba进行中文分词\n",
    "def chinese_segmentation(text):\n",
    "    words = jieba.cut(text, cut_all=False, HMM=True)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先处理缺失值，将缺失值替换为一个空字符串或其他合适的值\n",
    "df['content'].fillna('', inplace=True)\n",
    "# 对DataFrame的'content'列应用中文分词\n",
    "df['content_segmented'] = df['content'].apply(chinese_segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      前 几天 有 一句 十分 歹毒 的 骂人 话 冲 上 了 热 搜 ， 出处 是 正在 播出 ...\n",
       "1      一不小心 一口气 追剧 到   12   集 。 不说 一些 浮夸 的 剧情 ， 剧情 来自...\n",
       "2      今晚 看 《 新闻 女王 》 第   15   集 ， 很 有意思 。 文慧心 回到 家 ，...\n",
       "3      “ 找个 男人 嫁 了 吧 ” 比 以前   TVB   宫斗剧 扇 耳光 更 “ 狠 ” ...\n",
       "4      在 正式 看 这部 剧 之前 ， 光听 名字 就 觉得 像 个 烂剧 ， “ 女王 ” 两个...\n",
       "                             ...                        \n",
       "176    很久没 见 一部 剧 值得 给 长评 了 ， 这部 大约 是 值得 的 。 只是 等 了 这...\n",
       "177    文   \\   榴花 照图   \\   源自 网络 ， 侵删 致歉 ------------...\n",
       "178    《 新闻 女王 》 是 用 宫斗剧 的 打法 拍 现代 职场 剧 。 第十集 ， 文慧心 成...\n",
       "179    1 .   新闻 主播 英语 「 Anchor 」 \\n 原 解作 固定 船身 的 锚 \\n...\n",
       "180    今年 我 觉得 最 魔幻现实主义 的 事 发生 了 ， 当内 娱在 播放 《 以爱为 营 》...\n",
       "Name: content_segmented, Length: 181, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content_segmented']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 同义词替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      前 几天 有 一句 十分 歹毒 的 骂人 话 冲 上 了 热 搜 ， 出处 是 正在 播出 ...\n",
      "1      一不小心 一口气 追剧 到 12 集 。 不说 一些 浮夸 的 剧情 ， 剧情 来自 生活 ...\n",
      "2      今晚 看 《 新闻 女王 》 第 15 集 ， 很 有意思 。 文慧心 回到 家 ， 发现 ...\n",
      "3      “ 找个 男人 嫁 了 吧 ” 比 以前 TVB 宫斗剧 扇 耳光 更 “ 狠 ” 啊 ， ...\n",
      "4      在 正式 看 这部 剧 之前 ， 光听 名字 就 觉得 像 个 烂剧 ， “ 女王 ” 两个...\n",
      "                             ...                        \n",
      "176    很久没 见 一部 剧 值得 给 长评 了 ， 这部 大约 是 值得 的 。 只是 等 了 这...\n",
      "177    文 \\ 榴花 照图 \\ 源自 网络 ， 侵删 致歉 --------------------...\n",
      "178    《 新闻 女王 》 是 用 宫斗剧 的 打法 拍 现代 职场 剧 。 第十集 ， 文慧心 成...\n",
      "179    1 . 新闻 主播 英语 「 Anchor 」 原 解作 固定 船身 的 锚 引申 稳住 新...\n",
      "180    今年 我 觉得 最 魔幻现实主义 的 事 发生 了 ， 当内 娱在 播放 《 以爱为 营 》...\n",
      "Name: content_segmented, Length: 181, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 加载同义词文件\n",
    "synonym_file_path = 'synonym.txt'\n",
    "synonym_dict = {}\n",
    "\n",
    "# 读取同义词文件，创建同义词典，将同义词映射到主词\n",
    "with open(synonym_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) > 1:\n",
    "            main_word = parts[0]\n",
    "            synonyms = parts[1:]\n",
    "            for synonym in synonyms:\n",
    "                synonym_dict[synonym] = main_word\n",
    "\n",
    "# 替换同义词的函数（针对已分词的情况）\n",
    "def replace_synonyms(words):\n",
    "    # 使用同义词典替换同义词为主词\n",
    "    replaced_words = [synonym_dict.get(word, word) for word in words]\n",
    "\n",
    "    # 将分词结果拼接成字符串\n",
    "    return \" \".join(replaced_words)\n",
    "\n",
    "# 对 'content_segmented' 列应用替换同义词的函数\n",
    "df['content_segmented'] = df['content_segmented'].apply(lambda x: replace_synonyms(x.split()))\n",
    "\n",
    "# 显示替换同义词后的 DataFrame\n",
    "print(df['content_segmented'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 哪些影视作品被quote到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 提取包含在《》中的字符串的函数\n",
    "def extract_titles(text):\n",
    "    # 使用正则表达式提取《》中的字符串\n",
    "    matches = re.findall(r'《(.*?)》', text)\n",
    "    return matches\n",
    "\n",
    "# 创建新的 DataFrame df_movie\n",
    "df_movie = pd.DataFrame(columns=['Movie Title', 'Original Title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Title</th>\n",
       "      <th>Original Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Movie Title, Original Title]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie.to_excel(\"df_movie.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 drop_duplicates 方法去除同一 Original Title 下的重复 Movie Title\n",
    "df_unique_titles = df_movie[['Movie Title', 'Original Title']].drop_duplicates()\n",
    "\n",
    "# 统计 Movie Title 的频率\n",
    "title_frequency = df_unique_titles['Movie Title'].value_counts()\n",
    "\n",
    "# 打印结果或进行其他操作\n",
    "title_frequency.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词频分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          前\n",
       "1         几天\n",
       "2          有\n",
       "3         一句\n",
       "4         十分\n",
       "          ..\n",
       "100632    现实\n",
       "100633     的\n",
       "100634     了\n",
       "100635     吗\n",
       "100636     ？\n",
       "Name: content_segmented_list, Length: 100637, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将分好词的文本拆分成词语列表\n",
    "df['content_segmented_list'] = df['content_segmented'].str.split()\n",
    "\n",
    "# 使用explode将词语列表展开为单独的行\n",
    "df_tokens = df.explode('content_segmented_list')\n",
    "\n",
    "# 重新设置索引\n",
    "df_tokens = df_tokens.reset_index(drop=True)\n",
    "\n",
    "# 打印结果\n",
    "df_tokens.content_segmented_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content_segmented_list\n",
       "，      8872\n",
       "的      5484\n",
       "。      3175\n",
       "是      1684\n",
       "了      1236\n",
       "       ... \n",
       "散心        1\n",
       "常         1\n",
       "闭店        1\n",
       "店老板       1\n",
       "几十个       1\n",
       "Name: count, Length: 12898, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词频统计\n",
    "df_tokens[\"content_segmented_list\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导出数据\n",
    "df_tokens[\"content_segmented_list\"].value_counts().to_excel(\"reviews_freq.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import matutils\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# 预处理文本数据\n",
    "# 在这里，我们使用gensim的预处理函数，包括分词、去除停用词等\n",
    "\n",
    "# 步骤1：从外部文件读取停用词\n",
    "with open(\"stopwords.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    stop_words = file.read().splitlines()\n",
    "\n",
    "# 步骤2：将每一行的空格分隔单词转换为单词列表\n",
    "df['content_segmented_list'] = df['content_segmented'].apply(lambda x: x.split())\n",
    "\n",
    "# 步骤3：过滤停用词\n",
    "df['content_segmented_list'] = df['content_segmented_list'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# 创建字典和语料库\n",
    "dictionary = corpora.Dictionary(df['content_segmented_list'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['content_segmented_list']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义计算困惑度的函数\n",
    "def compute_perplexity(corpus, model):\n",
    "    return model.log_perplexity(corpus)\n",
    "\n",
    "# 定义计算主题一致性的函数\n",
    "def compute_coherence(corpus, model, dictionary, texts):\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "# 定义一个函数，用于绘制困惑度和主题一致性的图表\n",
    "def plot_metrics(num_topics_list, perplexity_values, coherence_values):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('Perplexity', color='tab:blue')\n",
    "    ax1.plot(num_topics_list, perplexity_values, color='tab:blue', marker='o')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Coherence', color='tab:red')\n",
    "    ax2.plot(num_topics_list, coherence_values, color='tab:red', marker='o')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title('LDA Model Evaluation Metrics')\n",
    "    plt.show()\n",
    "\n",
    "# 定义一个函数，用于训练LDA模型并计算评估指标\n",
    "def train_lda_model_and_evaluate(corpus, dictionary, texts, num_topics_list):\n",
    "    perplexity_values = []\n",
    "    coherence_values = []\n",
    "\n",
    "    for num_topics in num_topics_list:\n",
    "        # 训练LDA模型\n",
    "        lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "        # 计算困惑度\n",
    "        perplexity = compute_perplexity(corpus, lda_model)\n",
    "        perplexity_values.append(perplexity)\n",
    "\n",
    "        # 计算主题一致性\n",
    "        coherence = compute_coherence(corpus, lda_model, dictionary, texts)\n",
    "        coherence_values.append(coherence)\n",
    "\n",
    "    # 绘制图表\n",
    "    plot_metrics(num_topics_list, perplexity_values, coherence_values)\n",
    "\n",
    "# 选择要尝试的主题数量范围\n",
    "num_topics_list = list(range(1, 11))\n",
    "\n",
    "# 训练LDA模型并评估\n",
    "train_lda_model_and_evaluate(corpus, dictionary, df['content_segmented_list'], num_topics_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
